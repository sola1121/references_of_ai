1. 机器学习与测试数据集合
    人工智能, 机器学习通常都使用两组数据, 一组作为训练数据, 一组作为测试数据.
    每组数据中都包含一组多维的参数数据集作为特征数据集, 以及一组一维的数据作为结果分类数据, 从而形成4个数据集合.
    通常这4组数据变量的名称是:
        x_train, 训练数据, 多维参数数据集.
        y_train, 训练数据, 一维结果数据集.
        x_test, 测试数据, 多维参数数据集.
        y_test, 测试数据, 一维结果数据集.
    习惯上, train数据集用于训练, test数据集用于测试. 此外, 通过对数据集y_train的分析, 会生成一个新的预测结果数据集y_pred, 这个数据集意识一维的结果数据集.
    通过对结果数据集y_pred与实际的测试数据集y_test进行对比, 就可以检测算法模型的准确度.
    一般, x表参数, y表示结果; train表示训练用, test表示测试用.

2. 机器学习运行流程
    · 选择模型函数mx_fun, mx_fun是我们自定义的机器学习函数接口.
    · 把训练用的特征数据集x_train和对应的特征(结果)数据集y_train, 输入模型函数mx_fun.
    · 系统内置的机器学习函数, 会自动分析特征数据与结果数据之间的关系. 这样的一个过程就是机器学习的过程, 也是算法建模的过程.
    · 通过对训练数据的机器学习和数据分析, 系统会生成一个AI机器学习模型, 我们将其保存到变脸mx.
    · 把测试数据x_text输入模型变量mx, mx会调用内置的分析函数predict, 生成最终的分析结果y_pred.
    · 如果是实盘, 输入最新的数据, 例如今天的股市数据或正在销售的足彩比赛赔率数据, 系统会自动生成相关的预测数据.
    在进行实盘运行之前, 我们会对y_pred和正确结果数据y_test进行对比, 以判断模型的准确程度, 并通过一些优化措施和结果调整参数进行迭代运算, 或采用其他的模型提高最终结果的准确度.
    选择模型函数mx_fun --> 导入训练数据 --> 建立算法模型MX --> 输入测试(实盘)数据x_test --> 调用predict分析(预测)函数 --> 生成分析(预测)结果y_pred

3. 经典的机器学习算法
    在Sklearn模块库中
        线性回归算法, 函数名: LinearRegression
        朴素贝叶斯算法, Multinomial Naive Bayes, 函数名: Multinomialnb
        kNN邻近算法, 函数名: KNeighborsClassifier
        逻辑回归算法, 函数名: LogisticRegression
        随机森林算法, Random Forest Classifier, 函数名: RandomForestClassifier
        决策树算法, Decision Tree, 函数名: tree.DecisionTreeClassifier
        GBDT迭代决策树算法, Gradient Boosting Decision Tree, 又叫MART(Multiple Additive Regression Tree), 函数名: GradientBoostingClassifier
        SVM向量机算法, 函数名: SVC
        SVM-cross向量机交叉算法, 函数名: SVC


*机器学习概念的补充

监督学习是指在有标记的样本上建立机器学习模型. 而无监督学习则相反, 其面对的是没有标记的数据.

当需要用以昔日相似性指标对数据集进行分组时, 就可能会用到无监督学习.
最常见的无监督学习方法就是聚类. 当需要把无标记的数据分成几种集群时, 就需要用它来分析.

在机器学习领域中, 分类是指利用数据额特征将其分成若干类型的过程. 与回归输出实数结果不同, 监督学习分类器就是用带标记的训练数据建立一个模型, 然后对未知数据进行分类.

预测建模(predictive modeling)是一种用来预测系统未来行为的分析技术, 他由一群能够识别独立输入变量与反馈目标关联关系的算法构成.


*机器学习算法的补充

决策树是一个树状模型, 每个节点都做出一个决策, 从而影响结果. 叶子节点表示输出数值, 分支表示根据输入特征做出的中间决策. 

AdaBoost算法是指自适应增强(adaptive boosting)算法, 这是一种利用其他系统增强模型准确性的计数. 
这种技术是将不同版本的算法结果进行组合, 利用加权汇总的方式获得最终结果, 被称为弱机器学习(weak learners).
Adaboost算法在每个阶段获取的信息都会反馈到模型中, 这样学习器就可以在后一阶段重点训练难以分类的样本. 增强系统的准确性.

一般处理流程
首先使用Adaboost算法对数据集进行回归拟合, 在计算误差, 然后根误差评估结果, 用同样的数据集重新拟合. 
可以把这些看作是回归器的调优过程, 知道达到语气的准确性.

随机森林是一个决策树集合, 他基本上就是用一组由数据的若干子集构建的决策树构成, 再用决策树平均值改善整体学习效果.

逻辑分类回归是一种分类方法, 给定一组数据点, 需要建立一个可以在类之间绘制线性边界的模型. 逻辑回归就可以对训练数据派生的一组方程进行求解来提取边界.

SVM(support vecotr machines)是用来构建分类器和回归器的监督学习模型. SVM通过对数学方程组求解, 可以找出两组数据之间的最佳分割边界.

k-means 算法是最流行的聚类算法之一. 这个算法常常利用数据的不同属性将输入数据划分为k组. 分组使用最优化的技术实现的, 即让各组内的数据点与该组中心点的距离平方和最小化.

k-means 聚类的主要应用之一就是矢量量化. 简单来说, 矢量量化就是"四舍五入"的N维版本, 在处理数字等一维数据时, 会用四舍五入计数减少存储空间.
矢量量化被广泛应用于图片压缩, 使用比原始图像更少的比特数来存储每个像素, 从而实现图像图片.

均值漂移是一种强大的无监督学习方法, 用于集群数据点. 
该算法把数据点的分布看成是概率密度函数(probability-density function), 希望在特征空间中根据函数分布特征找出数据点的"模式(mode)". 
这些"模式"就对应于一群局部最密集(local maxima)分布的点. 均值漂移的算法优点是他不需要实现确定集群的数量.

层次聚类(hierarchical clustering)是一组聚类算法, 通过不断地分解或合并集群来构建树状集群(tree-likes clusters). 层次聚类的结构可以用一颗树表示
层次聚类算法可以是自下而上的, 也可以是自上而下的. 
在自下而上的算法中, 每个数据点都被看做是一个单独的集群. 这些集群不断地合并, 直到所有的集群都合并成一个巨型集群. 这被称为凝聚层次聚类.
与之相反的是, 自上而下的算法是从一个巨大的集群开始, 不断地分解, 直到所有的集群变成一个单独的数据点.

DBSCAN(Density Based Spatial Clustering of Applications with Noise, 带噪声的基于密度的聚类方法)
可以不用将集群数量当做一个参数输入.
DBSCAN将数据点看成是紧密集群的若干组. 如果某个点属于一个集群, 那么就应该有许多点页属于同一个集群. 
该方法里面有一个epsilon参数, 可以控制这个点到其他店的最大距离. 如果两个点的距离超过了参数epsilon的值, 他们就不可能在一个集群中.
该方法的主要优点是他可以处理异常点. 如果有一些点位于数据稀疏区域, DBSCAN就会把这些点作为异常点, 而不会强制将他们放入一个集群中.

近邻传播聚类(Affinity Propagation) 这种算法会找出数据中每个集群的代表性数据点, 会找到数据点间的相似性度量值, 并把所有数据点看成潜在的代表性数据点, 也称为取样器(exemplar)
